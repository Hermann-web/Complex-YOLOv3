# readme

The objectiv here is to test the complex yolo and of course test it on some kitti data i will chose

## setup env

```bash
# create a python3.8 env using any adaquate method
# activte the env
pip install uv
# uv pip install -r requirements.txt
uv pip install mayavi pyQt5 shapely opencv-python tqdm
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cpu
```

## load data

```bash
# download calibration data and labels
source data/run_download_and_unzip.sh
# put all data at the right place in the folder structure
# there ae some dependecies (velodyne files ans the images that were on a usb)
source data/kitti_data_prep.sh
# testout some predictions
python test_detection.py --split=sample --folder=sampledata  
# not tested
python test_both_side_detection.py --split=sample --folder=sampledata
# testout some predictions on my kitti files
python test_detection.py --split=sample2 --folder=2011_09_26_drive_0106_sync
```

## a code review

reading is code, the design pattern is very good. The implementation use, in a efficient ways old works. Still, there are some evident flaws in the code writing. For example, he created some files for kitti dataset but he could have avoided hard coding some variables into it.
He have a cool dataloader for his dataset.

He also has some utils function to help him in projections between (2D camera plane, 3D camera reference coordinates, 3D lidar reference coordinate)

Anyway, i know he use pretrained weights (yolov3 pretrained on some lidar bev map + labels).
Th ebev maps are generated by using as rgb, respectively, the density map, the height map and the intensity map (check the [function makeBVFeature of the file kitti_bev_utils](../utils/kitti_bev_utils.py) for the implementation)

To compute those maps, he get the unique (x,y,z) tuples from the point cloud. The density is the nb of repetitions of each point. He rescale using log and then normalize with log(64). The intensity map are the intensities of the actual unique points, same for the height map.

At inference, he feed the bev map to the model, get the predictions, run the latter through the projections to get boxes on a camera plane. In output, he use cv2 to show the image (+ boxes) and also show th ebev map with boxes

![images result](./images/img-result-on-kitti.png)

I could also save the images, then make it a video

## usage

The implementation use some module including the [yolov3 implmentation](https://github.com/eriklindernoren/PyTorch-YOLOv3) that GPL licensed so his code is also GPL licenses
But it is fait, i will use his code as a part of a whole used on top of other codes so i will not need to get my codebase GPL licensed but just his part

## make it a video

i have two feeds, bev and 2dcam
The input are the 40 first frames on a kitti sample

![bev video](./images/output/bev-output.gif)
`bev video`

![cam video](./images/output/img2d-output.gif)
`cam video`

## related links

- [another repo testing yolov3](https://github.com/eriklindernoren/PyTorch-YOLOv3)

## new tests for trcaking

```bash
uv pip install scipy numpy
pip install lap
pip install cython
pip install cython_bbox
```

### testing yolov5

I wanted to use my custom yolov3 weight like this

```python
MODEL_PATH = Path("checkpoints/tiny-yolov3_ckpt_epoch-220.pth")
use_cuda = True
device = "cuda" if torch.cuda.is_available() and use_cuda else "cpu"
checkpoint = torch.load(MODEL_PATH, map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

But it failed

So, i've loaded yolov5 from ultralytics repo directly into pytorch

```python
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=False) # or yolov5m, yolov5l, yolov5x, custom
```

It download the repo, make some updated in the python modules if necessary.

I've studied the prediction outputs (pytorch object) and adapted to use bytetrack class

```bash
python test_tracking_yolo.py 
```

<details>
<summary>output</summary>

Using cache found in /home/ubuntu/.cache/torch/hub/ultralytics_yolov5_master
YOLOv5 ðŸš€ 2024-4-1 Python-3.8.18 torch-2.2.0+cpu CPU

Fusing layers...
YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs
Adding AutoShape...
[OT_1_(1-1), OT_2_(1-1), OT_3_(1-1), OT_4_(1-1), OT_5_(1-1), OT_6_(1-1), OT_7_(1-1), OT_8_(1-1), OT_9_(1-1)]
[OT_1_(1-2), OT_2_(1-2), OT_3_(1-2), OT_4_(1-2), OT_5_(1-2), OT_6_(1-2), OT_7_(1-2), OT_8_(1-2)]
...
[OT_1_(1-36), OT_2_(1-36), OT_3_(1-36), OT_7_(1-36), OT_5_(1-36), OT_13_(3-36), OT_11_(3-36), OT_24_(26-36)]
[OT_1_(1-37), OT_2_(1-37), OT_3_(1-37), OT_7_(1-37), OT_5_(1-37), OT_13_(3-37), OT_10_(2-37), OT_26_(33-37), OT_4_(1-37), OT_6_(1-37), OT_12_(3-37)]
[OT_1_(1-38), OT_2_(1-38), OT_3_(1-38), OT_7_(1-38), OT_5_(1-38), OT_13_(3-38), OT_10_(2-38), OT_26_(33-38), OT_11_(3-38)]
[OT_1_(1-39), OT_2_(1-39), OT_3_(1-39), OT_7_(1-39), OT_5_(1-39), OT_13_(3-39), OT_10_(2-39), OT_26_(33-39), OT_28_(38-39), OT_11_(3-39)]
[OT_1_(1-40), OT_2_(1-40), OT_3_(1-40), OT_7_(1-40), OT_5_(1-40), OT_10_(2-40), OT_8_(1-40), OT_4_(1-40), OT_6_(1-40)]
</details>

The tracking is done. Now, i will process to visulaization

### testing yolov5 with image output

There again, i've copied the simplest experiment from bytetrack repo that did visualization and adapter the function internaly to use my prediction object

```bash
python test_tracking_yolo_vis.py --save_result --model yolov5
```

<details>
<summary>output</summary>
Using cache found in /home/ubuntu/.cache/torch/hub/ultralytics_yolov5_master
YOLOv5 ðŸš€ 2024-4-1 Python-3.8.18 torch-2.2.0+cpu CPU

Fusing layers...
YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs
Adding AutoShape...
Processing frame 20 (0.00 fps)
Processing frame 40 (0.00 fps)
save results to output/2024_04_03_11_45_20.txt
saved video to docs/images/output/yolo-track/cam-output-yolov5.avi
</details>

[the output video](./images/output/yolo-track/cam-output-yolov5.avi)

![output gif](./images/output/yolo-track/cam-output-yolov5.gif)

I can see the tracks in the images. Maybe i can be it a video or write from webcam lol

### testing yolov7

I wanted to use yolov8 like this

```python
model = torch.hub.load('ultralytics/yolov8', 'yolov8n', force_reload=False)
```

But it failed

In fact, ultralytics [didn't put it in support for torch.hub](https://github.com/ultralytics/ultralytics/issues/286#issuecomment-1480545123) as they prefer you use the `from ultrralitics import YOLO` method. Anyway, there was [an anwer](https://github.com/ultralytics/ultralytics/issues/286#issuecomment-1686405541) that use [a repo featuring yolov7](https://github.com/WongKinYiu/yolov7)

I just needed to download the weight

```bash
wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt
```

Then, load the model with the options `force_reload=False` and `trust_repo=True`

```python
model = torch.hub.load('WongKinYiu/yolov7', 'custom', 'yolov7-e6.pt', force_reload=False, trust_repo=True)
```

```bash
python test_tracking_yolo_vis.py --save_result --model yolov7
```

I have the tracks also

<details>
<summary>output</summary>
Using cache found in /home/ubuntu/.cache/torch/hub/WongKinYiu_yolov7_main

                 from  n    params  module                                  arguments                     
  0                -1  1         0  models.common.ReOrg                     []
  1                -1  1      8800  models.common.Conv                      [12, 80, 3, 1]
  2                -1  1     70880  models.common.DownC                     [80, 160, 1]
.................................................
.................................................
.................................................
.................................................
.................................................
134[-1, -2, -3, -4, -5, -6, -7, -8]  1         0  models.common.Concat                    [1]
135                -1  1   1639680  models.common.Conv                      [2560, 640, 1, 1]
136                99  1    461440  models.common.Conv                      [160, 320, 3, 1]
137               111  1   1844480  models.common.Conv                      [320, 640, 3, 1]
138               123  1   4149120  models.common.Conv                      [480, 960, 3, 1]
139               135  1   7375360  models.common.Conv                      [640, 1280, 3, 1]
140[136, 137, 138, 139]  1    817020  models.yolo.Detect                      [80, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [320, 640, 960, 1280]]
/home/ubuntu/Documents/GitHub/melint/lidar-object-classification-code-index/3rd-parties/Complex-YOLOv3/.venv/lib/python3.8/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Model Summary: 614 layers, 97246652 parameters, 97246652 gradients, 129.3 GFLOPS

Adding autoShape...
YOLOR ðŸš€ 2024-4-2 torch 2.2.0+cpu CPU

Processing frame 20 (0.00 fps)
Processing frame 40 (0.00 fps)
save results to output/2024_04_03_11_55_39.txt
saved video to docs/images/output/yolo-track/cam-output-yolov7.avi
</details>

[the output video](./images/output/yolo-track/cam-output-yolov7.avi)

![output gif](./images/output/yolo-track/cam-output-yolov7.gif)

## testing the tracker on pred on bev images

```bash
python test_tracking_complex_yolo.py --split=sample2 --folder=2011_09_26_drive_0106_sync --save_result --aspect_ratio_thresh=10
```

I've refactored the tracking process into a function that take as arg

```python
run_tracker_on_frame(
  frame_id:int, # id of the frame
  tracker:BYTETracker, # the bytrack instance
  detections, # array.shape=(nb_objects_detected, 5 or 6) = for each obj, x1,y1,x2,y2,score
  height:int, width:int, #dims of the bev image
  raw_img:np.ndarray, #the bev image (loaded from cv2 to array)
  aspect_ratio_thresh:float=1.6, #the greater, the more tracks 
  min_box_area:float=10, #the less, the more tracks
  timer:Timer=None #to take track
  )
```

[the output video](./images/output/complex-yolo-track/cam-output-complex-yolo.avi)

![](./images/output/complex-yolo-track/cam-output-complex-yolo.gif)

Track are found when augmenting aspect_ratio_thresh and reducing

I have some results but still far from industry acceptable, as i have only two tracks out of 17.

Ameliorations include :

- working on the tresholds `aspect_ratio_thresh`, `min_box_area`
- read on these treshholds, but essentily the bytetrack method
- try to level up using bot sort ?
- track on images instead of bev ?, track 3D bbox instead of just a rectangle ?

## tryout

```bash
python test_tracking_complex_yolo.py --split=sample2 --folder=2011_09_26_drive_0106_sync --save_result --aspect_ratio_thresh=10
```

Well, the problem was in the geometry
Draw 3d bounding box in image

```markdown
    qs: (8,3) array of vertices for the 3d box in following order:
        1 -------- 0
        /|         /|
      2 -------- 3 .
      | |        | |
      . 5 -------- 4
      |/         |/
      6 -------- 7
```

Here is the bounding box.
I did different mistakes

**coordinates integration:**

When sending x1y1x2y2 do the tracker, i consider the front plane so i send `x1,y1=front_left=P[2]` and `x2,y2=rear_right=P[7]`. Well, after the visu, i saw the plane i though was front, was actually behind

  ```python
  L = [2,6,7,3]
  print("box3d_pts_2d= ",box3d_pts_2d)
  x1, y1 = box3d_pts_2d[L[0]]
  x2, y2 = box3d_pts_2d[L[2]]
  ```

![2D bbox from 3Dbbox point selection](./images/box-exxp/part-boxes-example1.png)

It is easy to see when we compare to the 3D bbox

![origin 3D box](./images/box-exxp/full-boxes-example.png)

I tried to change the 2Dbbox to cover the objects

  ```python
  L = [2,5,4,3]
  print("box3d_pts_2d= ",box3d_pts_2d)
  x1, y1 = box3d_pts_2d[L[0]]
  x2, y2 = box3d_pts_2d[L[2]]
  ```

I've tried it on the img coordinate tracking and didnt get any result, like always.

![variation of the 2D bbox for more cover](./images/box-exxp/part-boxes-example2.png)

Judging by the result above, i didn't cover and and it makes sense

Further considerations, are consistant with a real visu that seammed like this instead

```markdown
    qs: (8,3) array of vertices for the 3d box in following order:
        6 -------- 7
        /|         /|
      5 -------- 4 .
      | |        | |
      . 2 -------- 3
      |/         |/
      1 -------- 0
```

I had some idea and did try swaping x1 and x2 on the fly. And i detected all object except for the two cars i only detected till now.

Now, i though i could choose wisely a plane, it should contains the corners: for examples, i can reserve as bottom left, `P1` is at t top right, `P7`.

And i can't choose a top left or a bottom right, so i thougn i will just use `x1,y1 = top_left= max(x),max(y)` and `x2,y2 = bottom_right=min(x),min(y)`

Well, that is dumb because the camera plane is like this: x, left to right and y, top to bottom

In fact, the camera plane look like below

```markdown
      0 ----x---->
      |
      y
      |
      |
```

so `x1,y1 = top_left = min(x), min(y)`, `x1,y1 = bottom_right = max(x), max(y)`

And i have the tracks, both on bev coordinate trackings and img coordinate trackings, using the same technique.

**rerun the script:**

```bash
python test_tracking_complex_yolo.py --split=sample2 --folder=2011_09_26_drive_0106_sync --save_result --aspect_ratio_thresh=10
```

results

- with `TEST_TRACKING = True`, `TEST_DETECTION = False`: [video output](./images/output/complex-yolo-track/cam-output-complex-yolo.avi)

  ![result of tracking on bev space](./images/output/complex-yolo-track/cam-output-complex-yolo.gif)

- with `TEST_TRACKING = False`, `TEST_DETECTION = False`: ![video output](./images/output/complex-yolo-track-img/cam-output-complex-yolo.avi)

  ![result of tracking on img space](./images/output/complex-yolo-track-img/cam-output-complex-yolo.gif)

## build poetry project

- save deps

```bash
cat requirements.txt | sed 's/ //g' | sed 's/[#>].*$//g' | grep -v "^$" | cut -d= -f1 | xargs -n 1 pip3 show | awk '/Name:|Version:/ {printf $2; if (/Name:/) printf "~="; else printf "\n"}' > docs/req/linux.venv38.req.txt
```

- create project

```bash
poetry init
```

- add source for pytorch

```bash
# for gpu
poetry source add --priority=explicit pytorch-gpu-src https://download.pytorch.org/whl/cu118
# or for cpu
poetry source add --priority=explicit pytorch-cpu https://download.pytorch.org/whl/cpu
```

- prep env and install

```bash
poetry use 3.8
poetry add torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --source pytorch-cpu
poetry add opencv-python tqdm
poetry add mayavi shapely
poetry add scipy==1.10.1
```

- export deps

```bash
poetry export -f requirements.txt --output docs/req/linux.poetry.req.txt --without-hashes

cat requirements.txt | sed 's/ //g' | sed 's/[#>].*$//g' | grep -v "^$" | cut -d= -f1 | xargs -n 1 pip3 show | awk '/Name:|Version:/ {printf $2; if (/Name:/) printf "~="; else printf "\n"}' > docs/req/linux.venv38.2.req.txt
```
